{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "import pytchat\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"michellejieli/emotion_text_classifier\")\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"michellejieli/emotion_text_classifier\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"michellejieli/emotion_text_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing MEED data for model\n",
    "MEED= prep_data(\"MEED\")\n",
    "train_data,val_data,frame_dim,max_x,min_x,max_y,min_y,max_dx,min_dx,max_dy,min_dy = MEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualise_skeleton(all_frames, max_x, max_y, max_frames=500, save=False, save_path=None, prefix=None, train_seed=None , delta=False, destroy = True):\n",
    "    \"\"\"Input all frames dim 50xn n being the number of frames 50= 25 keypoints x and y coordinates\"\"\"\n",
    "\n",
    "    \n",
    "    # visualise to check if the data is correct\n",
    "    # BODY_25 Keypoints\n",
    "    keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', \n",
    "                        'L-Elb', 'L-Wr', 'MidHip', 'R-Hip', 'R-Knee', 'R-Ank', \n",
    "                        'L-Hip', 'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', \n",
    "                        'L-Ear', 'L-BigToe', 'L-SmallToe', 'L-Heel', 'R-BigToe', \n",
    "                        'R-SmallToe', 'R-Heel']\n",
    "\n",
    "\n",
    "    limb_connections = [\n",
    "        (\"Nose\", \"Neck\"),\n",
    "        (\"Neck\", \"R-Sho\"),\n",
    "        (\"R-Sho\", \"R-Elb\"),\n",
    "        (\"R-Elb\", \"R-Wr\"),\n",
    "        (\"Neck\", \"L-Sho\"),\n",
    "        (\"L-Sho\", \"L-Elb\"),\n",
    "        (\"L-Elb\", \"L-Wr\"),\n",
    "        (\"Neck\", \"MidHip\"),\n",
    "        (\"MidHip\", \"R-Hip\"),\n",
    "        (\"R-Hip\", \"R-Knee\"),\n",
    "        (\"R-Knee\", \"R-Ank\"),\n",
    "        (\"MidHip\", \"L-Hip\"),\n",
    "        (\"L-Hip\", \"L-Knee\"),\n",
    "        (\"L-Knee\", \"L-Ank\"),\n",
    "        (\"Nose\", \"R-Eye\"),\n",
    "        (\"R-Eye\", \"R-Ear\"),\n",
    "        (\"Nose\", \"L-Eye\"),\n",
    "        (\"L-Eye\", \"L-Ear\"),\n",
    "        (\"L-Ank\", \"L-BigToe\"),\n",
    "        (\"L-Ank\", \"L-SmallToe\"),\n",
    "        (\"L-Ank\", \"L-Heel\"),\n",
    "        (\"R-Ank\", \"R-BigToe\"),\n",
    "        (\"R-Ank\", \"R-SmallToe\"),\n",
    "        (\"R-Ank\", \"R-Heel\")\n",
    "    ]\n",
    "    \n",
    "     # Define a mapping from emotion vectors to emotion labels\n",
    "    # Define emotion labels\n",
    "    emotion_labels = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sad', 'Surprise']\n",
    "    \n",
    "    # Initialize a blank canvas (image)\n",
    "    canvas_size = (int(max_y)+50, int(max_x)+50, 3)  \n",
    "    canvas = np.zeros(canvas_size, dtype=np.uint8)\n",
    "    \n",
    "    \n",
    "    if save:\n",
    "        # Determine the save path\n",
    "        if save_path is None:\n",
    "            save_path = f\"D:\\\\Interactive Dance Thesis Tests\\\\TransformerResults\\\\{train_seed}\"\n",
    "\n",
    "        # Ensure directory exists\n",
    "        if not os.path.exists(save_path):\n",
    "            print(f\"Creating directory {save_path}\")\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        # Determine a unique filename\n",
    "        existing_files = os.listdir(save_path)\n",
    "        file_num = 1\n",
    "        while f\"{prefix or ''}{file_num}.mp4\" in existing_files:\n",
    "            file_num += 1\n",
    "        out_path = os.path.join(save_path, f\"{prefix or ''}{file_num}.mp4\")\n",
    "\n",
    "        # Create the video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(out_path, fourcc, 10.0, (canvas_size[1], canvas_size[0]))\n",
    "    \n",
    "    previous_frame_data = None\n",
    "    \n",
    "    # Iterate over all frames; the first frame uses absolute keypoints, the rest use relative keypoints (deltas)\n",
    "    for frame_data in tqdm(all_frames[:max_frames], desc=\"Visualizing frames\"):\n",
    "        \n",
    "        # If previous_frame_data is None, this is the first frame and we use absolute positions.\n",
    "        # Otherwise, add the delta to the previous frame's keypoints to get the new keypoints\n",
    "        if delta ==True:\n",
    "            if previous_frame_data is not None:\n",
    "                frame_data[:50] = [prev + delta for prev, delta in zip(previous_frame_data[:50], frame_data[50:100])]\n",
    "        \n",
    "            # Update previous_frame_data\n",
    "            previous_frame_data = copy.deepcopy(frame_data)\n",
    "        \n",
    "        canvas_copy = canvas.copy()\n",
    "        \n",
    "        # Extract x, y coordinates and emotion vector\n",
    "        x_coords = frame_data[0:50:2] \n",
    "        y_coords = frame_data[1:50:2]\n",
    "        emotion_vector = tuple(frame_data[100:107])\n",
    "        \n",
    "        xy_coords = list(zip(x_coords, y_coords))\n",
    "        sane = sanity_check(xy_coords)\n",
    "        # Plot keypoints on the canvas\n",
    "        for i, (x, y) in enumerate(xy_coords):\n",
    "            if sane[i] == False:\n",
    "                continue\n",
    "            x_val = x.item() if torch.is_tensor(x) else x\n",
    "            y_val = y.item() if torch.is_tensor(y) else y\n",
    "            cv2.circle(canvas_copy, (int(x_val), int(y_val)), 3, (0, 0, 255), -1)  \n",
    "            cv2.putText(canvas_copy, keypointsMapping[i], (int(x_val), int(y_val)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Draw connections (limbs) on the canvas\n",
    "        for limb in limb_connections:\n",
    "            start_idx = keypointsMapping.index(limb[0])\n",
    "            end_idx = keypointsMapping.index(limb[1])\n",
    "            \n",
    "            start_point = (int(x_coords[start_idx]), int(y_coords[start_idx]))\n",
    "            end_point = (int(x_coords[end_idx]), int(y_coords[end_idx]))\n",
    "\n",
    "            if start_point == (0,0) or end_point == (0,0) or not sane[start_idx] or not sane[end_idx]:\n",
    "                continue\n",
    "            cv2.line(canvas_copy, start_point, end_point, (0, 255, 0), 2)  \n",
    "        \n",
    "        # Display the emotion percentages and labels on the top right of the frame\n",
    "        \n",
    "        emotion_percentages = [f\"{int(e * 100)}% {emotion_labels[i]}\" for i, e in enumerate(emotion_vector) if round(e * 100) > 1]\n",
    "        # print(emotion_percentages)\n",
    "        y0, dy = 30, 15  # Starting y position and line gap\n",
    "        for i, line in enumerate(emotion_percentages):\n",
    "            y = y0 + i * dy\n",
    "            cv2.putText(canvas_copy, line, (canvas_size[1] - 120, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        # Display the canvas with keypoints and connections\n",
    "        cv2.imshow(\"Keypoints Visualization\", canvas_copy)\n",
    "\n",
    "        # If saving, write the frame to the video\n",
    "        if save:\n",
    "            out.write(canvas_copy)\n",
    "\n",
    "        # Wait for 100ms and check for \"esc\" key press to exit\n",
    "        key = cv2.waitKey(100)\n",
    "        if key == 27:  \n",
    "            break\n",
    "\n",
    "    # Release the video writer, if used\n",
    "    if save:\n",
    "        out.release()\n",
    "\n",
    "    if destroy:\n",
    "        # Close the visualization window\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize a dictionary to keep track of cumulative scores and counts for each emotion\n",
    "emotion_data = {emotion: {\"score\": 0.0, \"count\": 0} for emotion in emotion_labels}\n",
    "\n",
    "\n",
    "# List to store average scores indexed by emotions\n",
    "average_scores = [0.0 for _ in emotion_labels]\n",
    "length =0\n",
    "\n",
    "BLOCK_SIZE = 16 # what is the maximum context length for predictions? \n",
    "\n",
    "# create model\n",
    "CHECKPOINT_PATH = \"checkpoints/proto6_checkpoint.pth\"\n",
    "\n",
    "m = MotionModel(input_dim=frame_dim, output_dim=frame_dim, blocksize=BLOCK_SIZE, hidden_dim=512, n_layers=8, dropout=DROPOUT)\n",
    "m = m.to(device)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG)  # weight_decay=1e-5 L2 regularization\n",
    "m, optimizer,scheduler, epoch, loss, train_seed = load_checkpoint(m, optimizer, CHECKPOINT_PATH)\n",
    "print(f\"training seed: {train_seed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chosen_emotion = \"Fear\"\n",
    "specific_emotion_video = get_video_by_emotion(train_data, chosen_emotion)\n",
    "specific_emotion_video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMES_GENERATE = 150\n",
    "\n",
    "chosen_emotion = \"Fear\"\n",
    "# seems like 150 frames is good before it get's a bit weird\n",
    "\n",
    "specific_emotion_video = get_video_by_emotion(train_data, chosen_emotion)\n",
    "\n",
    "# initial generation always neutral\n",
    "\n",
    "generated = m.generate(specific_emotion_video, FRAMES_GENERATE)\n",
    "unnorm_out = unnormalise_list_2D(generated, max_x, min_x, max_y, min_y,max_x, min_x, max_y, min_y)\n",
    "\n",
    "# visualise\n",
    "visualise_skeleton(unnorm_out[0], max_x, max_y, max_frames=FRAMES_GENERATE,save = True,save_path=None,prefix=f'{chosen_emotion}_{epoch}_coord',train_seed=train_seed,delta=False)\n",
    "\n",
    "\n",
    "# have to normalise the generated data before putting it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMES_GENERATE = 500\n",
    "\n",
    "\n",
    "noise = torch.randn(1, 5, 107).to(device)\n",
    "print(noise.shape)  # Should print: torch.Size([1, 97, 107])\n",
    "\n",
    "\n",
    "# initial generation always neutral\n",
    "\n",
    "generated = m.generate(noise, FRAMES_GENERATE)\n",
    "unnorm_out = unnormalise_list_2D(generated, max_x, min_x, max_y, min_y,max_x, min_x, max_y, min_y)\n",
    "\n",
    "# visualise\n",
    "visualise_skeleton(unnorm_out[0], max_x, max_y, max_frames=FRAMES_GENERATE,save = True,save_path=None,prefix=f'noise__{epoch}_coord',train_seed=train_seed,delta=False)\n",
    "\n",
    "\n",
    "# have to normalise the generated data before putting it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_generated(unnorm_out, max_x, min_x, max_y, min_y, max_dx, min_dx, max_dy, min_dy):\n",
    "    norm_out = []\n",
    "    \n",
    "    for frame in unnorm_out:\n",
    "        norm_frame = []\n",
    "        \n",
    "        # Normalize the first 50 values (absolute x and y coordinates)\n",
    "        for i in range(0, 50, 2):\n",
    "            unnormalized_x = frame[i]\n",
    "            unnormalized_y = frame[i+1]\n",
    "            \n",
    "            norm_x = 2 * (unnormalized_x - min_x) / (max_x - min_x) - 1\n",
    "            norm_y = 2 * (unnormalized_y - min_y) / (max_y - min_y) - 1\n",
    "            \n",
    "            norm_frame.extend([norm_x, norm_y])\n",
    "        \n",
    "        # Normalize the second 50 values (x and y deltas)\n",
    "        for i in range(50, 100, 2):\n",
    "            unnormalized_dx = frame[i]\n",
    "            unnormalized_dy = frame[i+1]\n",
    "            \n",
    "            norm_dx = 2 * (unnormalized_dx - min_dx) / (max_dx - min_dx) - 1\n",
    "            norm_dy = 2 * (unnormalized_dy - min_dy) / (max_dy - min_dy) - 1\n",
    "            \n",
    "            norm_frame.extend([norm_dx, norm_dy])\n",
    "        \n",
    "        # Append the emotion encoding without normalizing\n",
    "        norm_frame.extend(frame[-7:])\n",
    "        norm_out.append(norm_frame)\n",
    "        \n",
    "    return norm_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import keyboard\n",
    "import queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "files = glob.glob(\"D:/Interactive Dance Thesis Tests/TransformerResults/*/*500000*\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initial setup\n",
    "shared_data = {\n",
    "    'average_scores': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "chat = pytchat.create(video_id=\"gCNeDWCI0vo\")\n",
    "FRAMES_GENERATE = 150\n",
    "terminate_threads = False\n",
    "\n",
    "# This queue will hold the batches ready for visualization\n",
    "viz_queue = queue.Queue()\n",
    "\n",
    "\n",
    "def process_chat_message(c):\n",
    "    \"\"\"Process a chat message and update emotion scores.\"\"\"\n",
    "    print(f\"{c.datetime} [{c.author.name}]- {c.message}\")\n",
    "    result = pipe(c.message)  # Assuming pipe() returns emotion prediction\n",
    "    print(result)\n",
    "\n",
    "    detected_emotion = result[0]['label']\n",
    "\n",
    "    # Reset the counter for the detected emotion and boost its score\n",
    "    emotion_data[detected_emotion][\"count\"] = 0\n",
    "    score = result[0]['score']\n",
    "    emotion_data[detected_emotion][\"score\"] = min(1, emotion_data[detected_emotion][\"score\"] + score)\n",
    "\n",
    "    # Decay scores for other emotions and increase their counters\n",
    "    for emotion, data in emotion_data.items():\n",
    "        if emotion != detected_emotion:\n",
    "            data[\"count\"] += 1\n",
    "            if data[\"count\"] >= 5:\n",
    "                data[\"score\"] = 0\n",
    "            else:\n",
    "                data[\"score\"] *= 0.5  # or any other decay factor you prefer\n",
    "\n",
    "    # Update average scores\n",
    "    for i, emotion in enumerate(emotion_labels):\n",
    "        shared_data['average_scores'][i] = emotion_data[emotion][\"score\"]\n",
    "\n",
    "    print(\"Average scores:\", shared_data['average_scores'])\n",
    "\n",
    "\n",
    "INPUT_LENGTH = 1\n",
    "\n",
    "# Batch generation function\n",
    "def generate_new_batch():\n",
    "    \"\"\"Generate a new batch based on the current average scores.\"\"\"\n",
    "    last_frames = unnorm_out[0][-INPUT_LENGTH:]\n",
    "    norm_last_frames = normalise_generated(last_frames, max_x, min_x, max_y, min_y, max_x, min_x, max_y, min_y)\n",
    "    new_batch = [frame[:-7] + shared_data['average_scores'] for frame in norm_last_frames]\n",
    "    new_input = torch.tensor([new_batch]).to(device).float()\n",
    "\n",
    "    # Generate the new frames\n",
    "    generated = m.generate(new_input, FRAMES_GENERATE)\n",
    "    return unnormalise_list_2D(generated, max_x, min_x, max_y, min_y, max_x, min_x, max_y, min_y)\n",
    "\n",
    "# Periodic batch generation function - secs\n",
    "def generate_batches_periodically(period=2):\n",
    "    \"\"\"Periodically generate batches and put them in the visualization queue.\"\"\"\n",
    "    while True:\n",
    "        time.sleep(period)\n",
    "        unnorm_out = generate_new_batch()\n",
    "        viz_queue.put(unnorm_out)\n",
    "        \n",
    "def visualise(unnorm_out):\n",
    "    # visualize\n",
    "    visualise_skeleton(unnorm_out[0], max_x, max_y, max_frames=FRAMES_GENERATE,save = False,save_path=None,prefix=f'adam_{EPOCHS}_coord_switchhappytodisgust',train_seed=train_seed,delta=False,destroy=False)\n",
    "\n",
    "# Visualization thread\n",
    "def visualise_batches():\n",
    "    while True:\n",
    "        # Wait for a new batch to be available in the queue\n",
    "        unnorm_out = viz_queue.get()\n",
    "        \n",
    "        # visualise the batch\n",
    "        visualise(unnorm_out)\n",
    "\n",
    "# Start the threads\n",
    "threading.Thread(target=visualise_batches, daemon=True).start()\n",
    "threading.Thread(target=generate_batches_periodically, args=(10,), daemon=True).start()\n",
    "\n",
    "# Process chat messages\n",
    "length = 0\n",
    "while chat.is_alive():\n",
    "    if keyboard.is_pressed('esc'):  # Check if ESC key is pressed\n",
    "        terminate_threads = True\n",
    "        break  # Exit the main loop\n",
    "    for c in chat.get().sync_items():\n",
    "        process_chat_message(c)\n",
    "        length += 1\n",
    "        \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "yt = YouTube('https://www.youtube.com/watch?v=OlmMP2Z83Pg')\n",
    "yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution') .desc().first().download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_videos(csv_path, output_dir):\n",
    "    # Create the output directory if it does not exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path, header=None, names=[\"video_url\"])\n",
    "\n",
    "    # Iterate over the video URLs in the CSV file\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Downloading videos\"):\n",
    "        video_url = row['video_url']  # Assuming the column name is 'video_url'\n",
    "        video_name = os.path.basename(video_url)\n",
    "        output_path = os.path.join(output_dir, video_name)\n",
    "\n",
    "        # Download the video using wget\n",
    "        subprocess.run(['wget', '-O', output_path, video_url, '--show-progress'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"G:/Downloads/refined_10M_all_video_url.csv\"\n",
    "    output_dir = \"G:/UAL_Thesis/affective_computing_datasets/AIST\"\n",
    "    download_videos(csv_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob(\"G:\\\\UAL_Thesis\\\\affective_computing_datasets\\\\DanceDBrenders\\\\DanceDB\\\\*\\\\*_keypoints.txt\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_set = {\n",
    "    'Happy', 'Miserable', 'Relaxed', 'Sad', 'Satisfied', 'Tired', \n",
    "    'Excited', 'Afraid', 'Angry', 'Annoyed', 'Bored', 'Pleased',\n",
    "    'Neutral','Nervous','Mix','Curiosity'\n",
    "}\n",
    "\n",
    "\n",
    "emotion_labels = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sad', 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "emotions_set = {\n",
    "    'Happy', 'Miserable', 'Relaxed', 'Sad', 'Satisfied', 'Tired', \n",
    "    'Excited', 'Afraid', 'Angry', 'Annoyed', 'Bored', 'Pleased',\n",
    "    'Neutral', 'Nervous', 'Mix', 'Curiosity'\n",
    "}\n",
    "\n",
    "# Define the path pattern to load the files\n",
    "path_pattern = \"G:\\\\UAL_Thesis\\\\affective_computing_datasets\\\\DanceDBrenders\\\\DanceDB\\\\*\\\\*_keypoints.txt\"\n",
    "\n",
    "# Get the list of all files that match the pattern\n",
    "files = glob.glob(path_pattern)\n",
    "\n",
    "# Filter the files based on the keywords in emotions_set\n",
    "filtered_files = [file for file in files if any(emotion.lower() in file.lower() for emotion in emotions_set)]\n",
    "\n",
    "print(\"Filtered files:\")\n",
    "for file in filtered_files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "emotions_set = {\n",
    "    'Happy', 'Miserable', 'Relaxed', 'Sad', 'Satisfied', 'Tired', \n",
    "    'Excited', 'Afraid', 'Angry', 'Annoyed', 'Bored', 'Pleased',\n",
    "    'Neutral', 'Nervous', 'Mix', 'Curiosity'\n",
    "}\n",
    "\n",
    "# Define the path pattern to load the files\n",
    "path_pattern = \"G:\\\\UAL_Thesis\\\\affective_computing_datasets\\\\DanceDBrenders\\\\DanceDB\\\\*\\\\*_keypoints.txt\"\n",
    "\n",
    "# Get the list of all files that match the pattern\n",
    "files = glob.glob(path_pattern)\n",
    "\n",
    "# Filter the files based on the keywords in emotions_set and return the matched emotion\n",
    "filtered_files_and_emotions = [(file, emotion) for file in files for emotion in emotions_set if emotion.lower() in file.lower()]\n",
    "\n",
    "print(\"Filtered files and matched emotions:\")\n",
    "for file, emotion in filtered_files_and_emotions:\n",
    "    print(f\"{file}: {emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = filtered_files[0]\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "content_dict = json.load(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(content_dict)-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict['941']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "flattened_dict={}\n",
    "for i in tqdm(range(len(content_dict)-25)):\n",
    "    nested_list = content_dict[str(i)]\n",
    "     \n",
    "    flattened_dict[i] = [coordinate for sublist in nested_list for coordinate in (sublist if sublist is not None else [0, 0])]\n",
    "flattened_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "flattened_dict={}\n",
    "x=[]\n",
    "y=[]\n",
    "for i in tqdm(range(len(content_dict)-25)):\n",
    "    nested_list = content_dict[str(i)]\n",
    "    x.extend([coordinate[0] if coordinate is not None else 0 for coordinate in nested_list])\n",
    "    y.extend([coordinate[1] if coordinate is not None else 0 for coordinate in nested_list])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import *\n",
    "\n",
    "prep_data(dataset = \"DanceDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def danceDB_emotions():\n",
    "    # co pilot is really good at guessing emotions combinations\n",
    "    return {\n",
    "        'Happy': {'Happiness': 1.0},\n",
    "        'Miserable': {'Sad': 1.0},\n",
    "        'Relaxed': {'Neutral': 0.9, 'Happiness': 0.1},\n",
    "        'Sad': {'Sad': 1.0},\n",
    "        'Satisfied': {'Happiness': 1.0},\n",
    "        'Tired': {'Neutral': 1.0},\n",
    "        'Excited': {'Happiness': 0.9, 'Surprise': 0.1},\n",
    "        'Afraid': {'Fear': 1.0},\n",
    "        'Angry': {'Anger': 1.0},\n",
    "        'Annoyed': {'Anger': 0.5, 'Disgust': 0.5},\n",
    "        'Bored': {'Neutral': 0.8, 'Sad': 0.1, 'Anger': 0.1},\n",
    "        'Pleased': {'Happiness': 1.0},\n",
    "        'Neutral': {'Neutral': 1.0},\n",
    "        'Nervous': {'Fear': 0.5, 'Surprise': 0.5},\n",
    "        'Mix': {'Anger': 0.143, 'Disgust': 0.143, 'Fear': 0.143, 'Happiness': 0.143, 'Neutral': 0.143, 'Sad': 0.143, 'Surprise': 0.143},\n",
    "        'Curiosity': {'Surprise': 0.3,'Happiness': 0.3,'Neutral': 0.3,'Fear': 0.1}\n",
    "    }\n",
    "\n",
    "def encode_danceDB_emotion(emotion):\n",
    "    \n",
    "    emotion_labels = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sad', 'Surprise']\n",
    "    \n",
    "    emotion_mapping = danceDB_emotions()\n",
    "    if emotion not in emotion_mapping:\n",
    "        return \"Emotion not found in mapping\"\n",
    "    \n",
    "    encoding = [0.0] * len(emotion_labels)\n",
    "    for emotion_label, percentage in emotion_mapping[emotion].items():\n",
    "        if emotion_label in emotion_labels:\n",
    "            index = emotion_labels.index(emotion_label)\n",
    "            encoding[index] = percentage\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "# Test the function with an example\n",
    "example_emotion = 'Annoyed'\n",
    "encoded_emotion = encode_danceDB_emotion(example_emotion)\n",
    "print(f\"Encoding for '{example_emotion}': {encoded_emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(danceDB_emotions().keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interactive_dance_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
