# Prototype 3

## Week 6 & 7

Prototype 3 marks the first iteration of the transformer prototype, with the code accessible [here](notebooks/prototypes/inter-prototype-3.ipynb). This prototype was developed based on the NanoGPT transformer tutorial by [Andrej Karpathy](https://github.com/karpathy/minGPT), adapted for non-categorical data such as keypoints. Initially, the model exhibited issues, leading to a collapse, as evident from the results [here](https://drive.google.com/file/d/1Ka8TSQoIm8ml5755OfbYYREs0hgKP5ie/view?usp=drive_link).

The cause of this model collapse was unclear at first. However, I found that making the model deeper, while incorporating residual pathways and dropout layers to avert overfitting, appeared to mitigate the problem significantly.

During these two weeks, I also focused on sourcing an appropriate dataset for model testing. While my initial intention was to collect an emotion-labeled dance dataset, I pivoted to using emotion-labeled motion datasets from affective computing due to the scarcity of dance datasets. Several promising datasets were identified, including [VEATIC](http://arxiv.org/abs/2309.06745), [HEROES](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiKj8OUndOCAxX_VaQEHshqBcwQFnoECBIQAQ&url=https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F9922723&usg=AOvVaw33XuAW7Ku0XUE50_Qs6XPx&opi=89978449), and [MEED](https://www.nature.com/articles/s41597-023-02551-y). Ultimately, I selected MEED for its clean, well-organised data, and ample sample sise of approximately 4200 videos, each 97 frames long. Conveniently, MEED also utilises OpenPose BODY-25 keypoints, aligning with my implementation.

The transformer's input structure was designed as [Batch, Time, Keypoints+emotion], where 'Batch' is the number of videos, 'Time' refers to the number of frames, and 'Keypoints' includes 25 points, each with x and y coordinates, totalling 50 numbers. Additionally, an emotion vector is appended—a one-hot encoding of seven emotions: Anger, Disgust, Fear, Happiness, Sadness, Surprise, and Neutral. The transformer's output mirrors its input structure.

## Week 8

In week 8, I noticed significant instability in the facial keypoints, with values jumping erratically. Further examination of the raw data revealed that OpenPose defaults to 0,0 coordinates when keypoints are obscured or undetected, as shown [here](https://drive.google.com/file/d/1-FGQWCDSpwI2sYqSVO7cjgPPxFK3Ms8g/view?usp=drive_link). Addressing this in the transformer model proved challenging, as transformers cannot process NaN values. My initial approach was to implement a masking technique to make the model disregard 0,0 coordinates, but this solution was complex and ultimately ineffective.

Shifting focus to data preprocessing, I developed an algorithm to interpolate between coordinates when 0,0 values are encountered, which are invariably invalid. Crafting this algorithm was complex due to the variability in the occurrence of 0,0 coordinates—sometimes they appear in long sequences or at the beginning of a video, necessitating numerous case scenarios to be handled. Eventually, the algorithm successfully smoothed the keypoints, eliminating the erratic jumps. The week also involved extensive data preprocessing, such as normalising the data and stratifying the train, test, and validation sets by emotions to ensure balanced representation and avoid bias toward any specific emotion.
