{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototpye 4 Intermediate - Using MEED dataset in a transformer\n",
    "\n",
    "Use normalised keypoint values in a transformer.\n",
    "\n",
    "Adding emotional encoding. This is how it is going to work. Encoded emotions is going to be added to each frame, so that model is more flexible during generation.\n",
    "\n",
    "!The starting input is currently 1 long list of coords. But be careful because model might learn the transition between 1 video to another which is erroneous\n",
    "\n",
    "Front videos only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "import glob\n",
    "import os \n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Set root directory\n",
    "root_dir = \"C:\\\\Users\\\\avika\\\\OneDrive\\\\Documents\\\\UAL\\\\interactive_dance_thesis\"\n",
    "os.chdir(root_dir)\n",
    "\n",
    "# Check if the current working directory was set correctly\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1326/1326 [00:01<00:00, 1095.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1326\n",
      "1326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "from typing import List\n",
    "\n",
    "logger = logging.getLogger()\n",
    "# Clear previous handlers\n",
    "for handler in logger.handlers[:]:\n",
    "    handler.close()\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(filename= f\"preprocessing_log.txt\", level=logging.INFO, filemode='w')\n",
    "# logging clear file\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def interpolate(coord_prev, coord_next):\n",
    "    \"\"\"\n",
    "    Linearly interpolate between two coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    - coord_prev (float): Coordinate of the previous frame.\n",
    "    - coord_next (float): Coordinate of the next frame.\n",
    "\n",
    "    Returns:\n",
    "    - (float): Interpolated coordinate.\n",
    "    \"\"\"\n",
    "    return (coord_prev + coord_next) / 2\n",
    "\n",
    "def preprocess_data(files: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Pre-process data by interpolating to avoid (0,0) keypoints.\n",
    "\n",
    "    Parameters:\n",
    "    - files (List[str]): List of file paths to process.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Pre-processed data.\n",
    "    \"\"\"\n",
    "    x_list=[]\n",
    "    y_list=[]\n",
    "    conf_list=[]\n",
    "    emotions = []\n",
    "    \n",
    "    for file in tqdm(files):\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "            x = data['x']\n",
    "            y = data['y']\n",
    "            conf = data['confidence']\n",
    "            emotion_code = [file.split('_')[-2].split('\\\\')[0][3:-3]]\n",
    "            emotions.extend(emotion_code)\n",
    "            \n",
    "            if len(emotion_code) > 1:\n",
    "                print(emotion_code)\n",
    "            \n",
    "            for i in range(len(x)):\n",
    "                # Check if coordinate is (0,0)\n",
    "                if x[i] == 0 and y[i] == 0:\n",
    "                    # logger.info(f\"Found (0,0) at index {i} in file {file}\")\n",
    "                    # If first frame, copy from next frame\n",
    "                    if i == 0:\n",
    "                        j = i + 1\n",
    "                        # Find next non-(0,0) frame\n",
    "                        while x[j] == 0 and y[j] == 0:\n",
    "                            j += 1\n",
    "                        x[i] = x[j]\n",
    "                        y[i] = y[j]\n",
    "                    # If last frame, copy from previous frame\n",
    "                    elif i == len(x) - 1:\n",
    "                        x[i] = x[i-1]\n",
    "                        y[i] = y[i-1]\n",
    "                    # For a frame in the middle\n",
    "                    else:\n",
    "                        # Find the next non-(0,0) frame\n",
    "                        j = i + 1\n",
    "                        while j < len(x) and x[j] == 0 and y[j] == 0:\n",
    "                            j += 1\n",
    "                        # If no non-(0,0) frame found, use the previous frame, otherwise interpolate\n",
    "                        if j == len(x):\n",
    "                            x[i] = x[i-1]\n",
    "                            y[i] = y[i-1]\n",
    "                        else:\n",
    "                            x[i] = interpolate(x[i-1], x[j])\n",
    "                            y[i] = interpolate(y[i-1], y[j])\n",
    "            \n",
    "            x_list.append(x)\n",
    "            y_list.append(y)\n",
    "            conf_list.append(conf)\n",
    "\n",
    "    return {\"x\": x_list, \"y\": y_list, \"confidence\": conf_list, \"emotions\": emotions}\n",
    "\n",
    "\n",
    "files = glob.glob(\"G:/UAL_Thesis/affective_computing_datasets/multiview-emotional-expressions-dataset/*/front_*/processed_data.json\")\n",
    "processed_data = preprocess_data(files)\n",
    "x_list = processed_data['x']\n",
    "y_list = processed_data['y']\n",
    "conf_list = processed_data['confidence']\n",
    "emotions_labels = processed_data['emotions']\n",
    "\n",
    "print(len(x_list))\n",
    "print(len(emotions_labels))\n",
    "# shape [num_frames, keypoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1717"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotions_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('A',Anger,'D',Disgust,'F',Fear,'H',Happiness,'N',Neutral,'SA',Sad,'SU',Surprise);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are still 0,0 coordinates\n",
    "\n",
    "for i in range(len(x_list)):\n",
    "    for j in range(len(x_list[i])):\n",
    "        if x_list[i][j] == 0 and y_list[i][j] == 0:\n",
    "            print(i,j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1326"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_values_2D(frames):\n",
    "    \"\"\"\n",
    "    Takes in a list of lists (frames), returns max and min values and normalized list\n",
    "    \n",
    "    Parameters:\n",
    "        frames: List of lists containing keypoints for each frame.\n",
    "    \n",
    "    Returns:\n",
    "        max_val: Maximum keypoint value across all frames.\n",
    "        min_val: Minimum keypoint value across all frames.\n",
    "        normalized_frames: Normalized keypoints for each frame.\n",
    "    \"\"\"\n",
    "    # Flatten the data to find global min and max\n",
    "    flat_data = [kp for frame in frames for kp in frame]\n",
    "    max_val = max(flat_data)\n",
    "    min_val = min(flat_data)\n",
    "    \n",
    "    # Normalize data\n",
    "    normalized_frames = [\n",
    "        [2 * (kp - min_val) / (max_val - min_val) - 1 for kp in frame]\n",
    "        for frame in frames\n",
    "    ]\n",
    "    \n",
    "    return max_val, min_val, normalized_frames\n",
    "\n",
    "max_x, min_x, normalised_x = normalize_values_2D(x_list)\n",
    "max_y, min_y, normalised_y = normalize_values_2D(y_list)\n",
    "len(normalised_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # create 1D array of 50 numbers (x,y,x,y --> 25 keypoints) for each frame\n",
    "# all_frames = []\n",
    "# n_parts = 25\n",
    "\n",
    "# for i in tqdm(range(0, len(x_list), n_parts)):\n",
    "#     frame_data = [coord for pair in zip(x_list[i:i+n_parts], y_list[i:i+n_parts]) for coord in pair]\n",
    "#     all_frames.append(frame_data)\n",
    "\n",
    "# data has 0s - need to ignore\n",
    "\n",
    "def visualise_skeleton(all_frames, max_x, max_y, max_frames=500, save=False, save_path=None, prefix=None):\n",
    "    \n",
    "    \"\"\"Input all frames dim 50xn n being the number of frames 50= 25 keypoints x and y coordinates\"\"\"\n",
    "\n",
    "    \n",
    "    # visualise to check if the data is correct\n",
    "    # BODY_25 Keypoints\n",
    "    keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', \n",
    "                        'L-Elb', 'L-Wr', 'MidHip', 'R-Hip', 'R-Knee', 'R-Ank', \n",
    "                        'L-Hip', 'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', \n",
    "                        'L-Ear', 'L-BigToe', 'L-SmallToe', 'L-Heel', 'R-BigToe', \n",
    "                        'R-SmallToe', 'R-Heel']\n",
    "\n",
    "\n",
    "    limb_connections = [\n",
    "        (\"Nose\", \"Neck\"),\n",
    "        (\"Neck\", \"R-Sho\"),\n",
    "        (\"R-Sho\", \"R-Elb\"),\n",
    "        (\"R-Elb\", \"R-Wr\"),\n",
    "        (\"Neck\", \"L-Sho\"),\n",
    "        (\"L-Sho\", \"L-Elb\"),\n",
    "        (\"L-Elb\", \"L-Wr\"),\n",
    "        (\"Neck\", \"MidHip\"),\n",
    "        (\"MidHip\", \"R-Hip\"),\n",
    "        (\"R-Hip\", \"R-Knee\"),\n",
    "        (\"R-Knee\", \"R-Ank\"),\n",
    "        (\"MidHip\", \"L-Hip\"),\n",
    "        (\"L-Hip\", \"L-Knee\"),\n",
    "        (\"L-Knee\", \"L-Ank\"),\n",
    "        (\"Nose\", \"R-Eye\"),\n",
    "        (\"R-Eye\", \"R-Ear\"),\n",
    "        (\"Nose\", \"L-Eye\"),\n",
    "        (\"L-Eye\", \"L-Ear\"),\n",
    "        (\"L-Ank\", \"L-BigToe\"),\n",
    "        (\"L-Ank\", \"L-SmallToe\"),\n",
    "        (\"L-Ank\", \"L-Heel\"),\n",
    "        (\"R-Ank\", \"R-BigToe\"),\n",
    "        (\"R-Ank\", \"R-SmallToe\"),\n",
    "        (\"R-Ank\", \"R-Heel\")\n",
    "    ]\n",
    "    \n",
    "     # Define a mapping from emotion vectors to emotion labels\n",
    "    # Define emotion labels\n",
    "    emotion_labels = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sad', 'Surprise']\n",
    "    \n",
    "    # Initialize a blank canvas (image)\n",
    "    canvas_size = (int(max_y)+50, int(max_x)+50, 3)  \n",
    "    canvas = np.zeros(canvas_size, dtype=np.uint8)\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    if save:\n",
    "        # Determine the save path\n",
    "        if save_path is None:\n",
    "            save_path = \"D:\\\\Interactive Dance Thesis Tests\\\\TransformerResults\"\n",
    "\n",
    "        # Ensure directory exists\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        # Determine a unique filename\n",
    "        existing_files = os.listdir(save_path)\n",
    "        file_num = 1\n",
    "        while f\"{prefix or ''}{file_num}.mp4\" in existing_files:\n",
    "            file_num += 1\n",
    "        out_path = os.path.join(save_path, f\"{prefix or ''}{file_num}.mp4\")\n",
    "\n",
    "        # Create the video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(out_path, fourcc, 10.0, (canvas_size[1], canvas_size[0]))\n",
    "\n",
    "    # Iterate over every frame\n",
    "    for frame_data in all_frames[:max_frames]:\n",
    "        canvas_copy = canvas.copy()\n",
    "\n",
    "        # Extract x and y coordinates\n",
    "        x_coords = frame_data[0:50:2] \n",
    "        y_coords = frame_data[1:50:2]\n",
    "        emotion_vector = tuple(frame_data[-7:])\n",
    "        \n",
    "\n",
    "        # Get emotion percentages and labels\n",
    "        emotion_percentages = [f\"{int(e * 100)}% {label}\" for e, label in zip(emotion_vector, emotion_labels) if e > 0]\n",
    "\n",
    "\n",
    "        # Plot keypoints on the canvas\n",
    "        for i, (x, y) in enumerate(zip(x_coords, y_coords)):\n",
    "            x_val = x.item() if torch.is_tensor(x) else x\n",
    "            y_val = y.item() if torch.is_tensor(y) else y\n",
    "            cv2.circle(canvas_copy, (int(x_val), int(y_val)), 3, (0, 0, 255), -1)  \n",
    "            cv2.putText(canvas_copy, keypointsMapping[i], (int(x_val), int(y_val)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        # Draw connections (limbs) on the canvas\n",
    "        for limb in limb_connections:\n",
    "            start_point = (int(x_coords[keypointsMapping.index(limb[0])]), int(y_coords[keypointsMapping.index(limb[0])]))\n",
    "            end_point = (int(x_coords[keypointsMapping.index(limb[1])]), int(y_coords[keypointsMapping.index(limb[1])]))\n",
    "\n",
    "            if start_point == (0,0) or end_point == (0,0):\n",
    "                continue\n",
    "            cv2.line(canvas_copy, start_point, end_point, (0, 255, 0), 2)  \n",
    "        \n",
    "        # Display the emotion percentages and labels on the top right of the frame\n",
    "        y0, dy = 30, 15  # Starting y position and line gap\n",
    "        for i, line in enumerate(emotion_percentages):\n",
    "            y = y0 + i * dy\n",
    "            cv2.putText(canvas_copy, line, (canvas_size[1] - 120, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        # Display the canvas with keypoints and connections\n",
    "        cv2.imshow(\"Keypoints Visualization\", canvas_copy)\n",
    "\n",
    "        # If saving, write the frame to the video\n",
    "        if save:\n",
    "            out.write(canvas_copy)\n",
    "\n",
    "        # Wait for 100ms and check for \"esc\" key press to exit\n",
    "        key = cv2.waitKey(100)\n",
    "        if key == 27:  \n",
    "            break\n",
    "\n",
    "    # Release the video writer, if used\n",
    "    if save:\n",
    "        out.release()\n",
    "\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1326/1326 [00:00<00:00, 2437.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create 1D array of 50 numbers (x,y,x,y --> 25 keypoints) for each frame\n",
    "kp_frames = []\n",
    "n_parts = 25\n",
    "\n",
    "for i in tqdm(range(0, len(normalised_x))):\n",
    "    video_x = normalised_x[i]\n",
    "    video_y = normalised_y[i]\n",
    "    kp_frame= []\n",
    "    for j in range(0,len(video_x), n_parts):\n",
    "        frame_data = [coord for pair in zip(video_x[j:j+n_parts], video_y[j:j+n_parts]) for coord in pair]\n",
    "        kp_frame.append(frame_data)\n",
    "    kp_frames.append(kp_frame)\n",
    "\n",
    "len(kp_frames[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def emotion_labels_to_vectors(emotion_labels):\n",
    "    \"\"\"\n",
    "    Convert a list of emotion labels to a list of continuous emotion vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - emotion_labels (list of str): A list of emotion labels.\n",
    "    \n",
    "    Returns:\n",
    "    - list of np.array: A list of continuous emotion vectors.\n",
    "    \"\"\"\n",
    "    # Define a mapping from emotion labels to emotion vectors\n",
    "    label_to_vector = {\n",
    "        'A': [1, 0, 0, 0, 0, 0, 0],   # Anger\n",
    "        'D': [0, 1, 0, 0, 0, 0, 0],   # Disgust\n",
    "        'F': [0, 0, 1, 0, 0, 0, 0],   # Fear\n",
    "        'H': [0, 0, 0, 1, 0, 0, 0],   # Happiness\n",
    "        'N': [0, 0, 0, 0, 1, 0, 0],   # Neutral\n",
    "        'SA': [0, 0, 0, 0, 0, 1, 0],  # Sad\n",
    "        'SU': [0, 0, 0, 0, 0, 0, 1]   # Surprise\n",
    "    }\n",
    "    \n",
    "    # Convert the labels to vectors using the mapping\n",
    "    emotion_vectors = [label_to_vector[label] for label in emotion_labels]\n",
    "    \n",
    "    return emotion_vectors\n",
    "\n",
    "# Convert labels to vectors\n",
    "emotion_vectors = emotion_labels_to_vectors(emotions_labels)\n",
    "emotion_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kp_frames[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1326/1326 [00:00<00:00, 33678.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kp_frames_with_emotion = []\n",
    "# kp_frames is normalised\n",
    "\n",
    "for i in tqdm(range(len(emotion_vectors))):\n",
    "    # Use list concatenation instead of extend() to avoid in-place modification and None\n",
    "    for frame in kp_frames[i]:\n",
    "        frame.extend(emotion_vectors[i])\n",
    "    kp_frames_with_emotion.append(kp_frames[i])\n",
    "    \n",
    "\n",
    "len(kp_frames_with_emotion[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's now split up the data into train and validation sets !FIX ALL VALIDATION BATCHES ARE ANGRY\n",
    "# n = int(0.9*len(kp_frames_with_emotion)) # first 90% will be train, rest val\n",
    "# train_data = kp_frames_with_emotion[:n]\n",
    "# val_data = kp_frames_with_emotion[n:]\n",
    "import random\n",
    "\n",
    "def stratified_split(data, test_size=0.1):\n",
    "    # Organize data by class\n",
    "    class_data = {}\n",
    "    for video_index, video in enumerate(data):\n",
    "        # Assume the last 7 elements of the first frame of each video represent the class (emotion)\n",
    "        emotion = tuple(video[0][-7:])  \n",
    "        if emotion not in class_data:\n",
    "            class_data[emotion] = []\n",
    "        class_data[emotion].append(video_index)  # Store video index instead of data to save memory\n",
    "\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "\n",
    "    # For each class, split the data into train and val sets\n",
    "    for emotion, video_indices in class_data.items():\n",
    "        random.shuffle(video_indices)  # Shuffle indices to ensure random splits\n",
    "        split_idx = int(len(video_indices) * (1 - test_size))  # Index to split train and val\n",
    "        train_indices.extend(video_indices[:split_idx])\n",
    "        val_indices.extend(video_indices[split_idx:])\n",
    "\n",
    "    # Retrieve the data using the indices\n",
    "    train_data = [data[idx] for idx in train_indices]\n",
    "    val_data = [data[idx] for idx in val_indices]\n",
    "\n",
    "    # Shuffle the train and val sets to ensure random order\n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(val_data)\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "train_data, val_data = stratified_split(kp_frames_with_emotion, test_size=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([8, 10, 57])\n",
      "tensor([[[ 1.2298e-01, -5.0277e-01,  1.0782e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1807e-01, -5.0279e-01,  1.0790e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1802e-01, -5.0275e-01,  1.0786e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 1.1300e-01, -5.0298e-01,  1.0279e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1283e-01, -5.0286e-01,  1.0260e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0790e-01, -4.9715e-01,  9.7573e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 3.8961e-01, -3.0988e-01,  3.0775e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.8461e-01, -3.0413e-01,  3.0786e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.8984e-01, -3.1020e-01,  3.0773e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 3.6409e-01, -3.5677e-01,  2.9755e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.5888e-01, -3.5669e-01,  2.8196e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.4885e-01, -3.5078e-01,  2.6175e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-8.7144e-02, -3.1583e-01, -4.6073e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-8.7073e-02, -3.1588e-01, -4.5979e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-7.1828e-02, -3.1596e-01, -3.5784e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-3.0832e-02, -3.2170e-01, -1.0109e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-2.0332e-02, -3.2192e-01, -7.5349e-05,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.5352e-02, -3.2197e-01,  5.2797e-03,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.1672e-02, -3.3938e-01,  6.6547e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.1686e-02, -3.3938e-01,  6.6596e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.1686e-02, -3.3938e-01,  6.6579e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 6.1718e-02, -3.3932e-01,  6.6596e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.1690e-02, -3.3929e-01,  6.6642e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.1725e-02, -3.3935e-01,  6.6614e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.7418e-01, -3.2743e-01,  1.7448e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.7962e-01, -3.2744e-01,  1.7948e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.9487e-01, -3.2751e-01,  1.7961e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 2.1527e-01, -3.5094e-01,  1.5402e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 2.1550e-01, -3.5678e-01,  1.4382e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 2.2071e-01, -3.6248e-01,  1.3861e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 2.5680e-02, -4.0956e-01,  3.5910e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-8.5863e-05, -3.9765e-01,  2.0675e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.0270e-02, -3.8611e-01,  1.5545e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 5.1851e-03, -4.3860e-01,  1.5510e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.5524e-02, -4.4454e-01,  2.0514e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 2.5775e-02, -4.4453e-01,  2.5655e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([8, 10, 57])\n",
      "tensor([[[ 1.1807e-01, -5.0279e-01,  1.0790e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1802e-01, -5.0275e-01,  1.0786e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.1794e-01, -5.0274e-01,  1.0782e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 1.1283e-01, -5.0286e-01,  1.0260e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0790e-01, -4.9715e-01,  9.7573e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0779e-01, -4.9124e-01,  9.7507e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 3.8461e-01, -3.0413e-01,  3.0786e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.8984e-01, -3.1020e-01,  3.0773e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.7954e-01, -3.0990e-01,  3.0760e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 3.5888e-01, -3.5669e-01,  2.8196e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.4885e-01, -3.5078e-01,  2.6175e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.4340e-01, -3.5080e-01,  2.4115e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-8.7073e-02, -3.1588e-01, -4.5979e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-7.1828e-02, -3.1596e-01, -3.5784e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-6.6603e-02, -3.1599e-01, -3.0709e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [-2.0332e-02, -3.2192e-01, -7.5349e-05,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.5352e-02, -3.2197e-01,  5.2797e-03,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-5.0799e-03, -3.2743e-01,  1.5387e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.1686e-02, -3.3938e-01,  6.6596e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.1686e-02, -3.3938e-01,  6.6579e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.1655e-02, -3.3928e-01,  6.6617e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 6.1690e-02, -3.3929e-01,  6.6642e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.1725e-02, -3.3935e-01,  6.6614e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 6.1690e-02, -3.3929e-01,  6.6561e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.7962e-01, -3.2744e-01,  1.7948e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 1.9487e-01, -3.2751e-01,  1.7961e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 2.0515e-01, -3.2778e-01,  1.7961e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 2.1550e-01, -3.5678e-01,  1.4382e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 2.2071e-01, -3.6248e-01,  1.3861e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 2.2547e-01, -3.6267e-01,  1.3834e-01,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-8.5863e-05, -3.9765e-01,  2.0675e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.0270e-02, -3.8611e-01,  1.5545e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [-1.5404e-02, -3.9199e-01,  1.0130e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 1.5524e-02, -4.4454e-01,  2.0514e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 2.5775e-02, -4.4453e-01,  2.5655e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 3.5952e-02, -4.3295e-01,  3.0839e-02,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]]], device='cuda:0')\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(1337)\n",
    "def get_batch(split, block_size, batch_size, device=device):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Choose random videos\n",
    "    ix = torch.randint(len(data), (batch_size,))\n",
    "\n",
    "    # For each chosen video, select a random starting point\n",
    "    start_frames = [torch.randint(len(data[i]) - block_size, (1,)).item() for i in ix]\n",
    "\n",
    "    # Extract subsequences from each chosen video and convert to tensors\n",
    "    x = torch.stack([torch.tensor(data[i][start:start+block_size], dtype=torch.float32) for i, start in zip(ix, start_frames)])\n",
    "    y = torch.stack([torch.tensor(data[i][start+1:start+block_size+1], dtype=torch.float32) for i, start in zip(ix, start_frames)])\n",
    "\n",
    "    # Compute the mask to mask out -inf values\n",
    "    mask = (x != float('-inf')).all(dim=-1).float()  # assuming -inf is present in any part of the data point\n",
    "\n",
    "    # Move tensors to the designated device\n",
    "    x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "    \n",
    "    return x, y, mask\n",
    "block_size = 10  # example block size\n",
    "batch_size = 8   # example batch size\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # example device\n",
    "\n",
    "# Get a batch from training data (assuming data is a nested list)\n",
    "xb, yb, mask = get_batch('train',  block_size, batch_size, device)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_list = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sad', 'Surprise']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotions are consistent between x and y.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def validate_emotion_consistency(x, y):\n",
    "    \"\"\"\n",
    "    Validate that the emotion code (last 7 elements of each frame) is consistent\n",
    "    between corresponding frames in x and y.\n",
    "\n",
    "    Parameters:\n",
    "    - x (Tensor): Input sequences (batch_size, sequence_length, frame_length)\n",
    "    - y (Tensor): Target sequences (batch_size, sequence_length, frame_length)\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if emotions are consistent, False otherwise\n",
    "    \"\"\"\n",
    "    # Extract the emotion encodings from x and y\n",
    "    emotion_x = x[:, :, -7:]\n",
    "    emotion_y = y[:, :, -7:]\n",
    "\n",
    "    # Check if the emotion encodings are equal in x and y\n",
    "    emotion_equal = torch.all(emotion_x == emotion_y, dim=-1)\n",
    "    \n",
    "    # Check equality across sequence length (assuming dim 1 is sequence_length)\n",
    "    emotion_equal = torch.all(emotion_equal, dim=-1)\n",
    "\n",
    "    # Check if all batches have consistent emotions\n",
    "    all_equal = torch.all(emotion_equal)\n",
    "\n",
    "    return all_equal.item()\n",
    "\n",
    "\n",
    "# Example tensors (ensure your actual tensors match these dimensions)\n",
    "\n",
    "\n",
    "# Validate emotion consistency\n",
    "is_consistent = validate_emotion_consistency(xb, yb)\n",
    "\n",
    "# Output result\n",
    "if is_consistent:\n",
    "    print(\"Emotions are consistent between x and y.\")\n",
    "else:\n",
    "    print(\"Emotions are NOT consistent between x and y.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 57])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10, 57])\n",
      "tensor([[[-0.2506,  0.2562,  0.5141,  ...,  0.0398,  0.3059, -0.5223],\n",
      "         [ 0.1192, -0.0865,  0.5458,  ...,  0.1515,  0.3803, -0.0082],\n",
      "         [ 0.0063,  0.0775,  0.4279,  ...,  0.1000,  0.3039, -0.1170],\n",
      "         ...,\n",
      "         [-0.3719,  0.5620,  0.3835,  ...,  0.2106,  0.0203, -0.3203],\n",
      "         [-0.2227,  0.3571,  0.1396,  ...,  0.3127, -0.2925, -0.1949],\n",
      "         [-0.4657,  0.6440,  0.2054,  ...,  0.1114,  0.0118, -0.1403]],\n",
      "\n",
      "        [[-0.0282,  0.0945,  0.4188,  ...,  0.0166,  0.1470, -0.0568],\n",
      "         [ 0.0476,  0.2740,  0.2742,  ...,  0.0865, -0.0378, -0.6497],\n",
      "         [-0.4341,  0.4094,  0.4140,  ...,  0.3239,  0.2361, -0.0909],\n",
      "         ...,\n",
      "         [-0.0469,  0.7431,  0.1787,  ...,  0.0942,  0.3745, -0.4545],\n",
      "         [-0.3317,  0.3066,  0.4727,  ...,  0.1354, -0.0514, -0.0748],\n",
      "         [-0.4803,  0.7580,  0.1955,  ...,  0.0214, -0.0547, -0.2634]],\n",
      "\n",
      "        [[ 0.4023,  0.2810,  0.3993,  ...,  0.2420,  0.2251,  0.0415],\n",
      "         [-0.2512,  0.3505,  0.6572,  ...,  0.0595,  0.3554, -0.5397],\n",
      "         [-0.0790,  0.4978,  0.5671,  ...,  0.0084,  0.1841, -0.2367],\n",
      "         ...,\n",
      "         [-0.1943,  0.7622,  0.0084,  ...,  0.4485,  0.2245, -0.2601],\n",
      "         [-0.3146,  0.4319, -0.0597,  ...,  0.2903,  0.1241, -0.4241],\n",
      "         [-0.1259,  0.4338,  0.1178,  ...,  0.3086,  0.4397, -0.3757]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1709,  0.3385,  0.3136,  ...,  0.0212,  0.0690,  0.0954],\n",
      "         [-0.1254,  0.1241,  0.1732,  ...,  0.0562,  0.1759, -0.1483],\n",
      "         [-0.0323,  0.2597,  0.5475,  ..., -0.0794,  0.2482,  0.0343],\n",
      "         ...,\n",
      "         [-0.0195,  0.4190,  0.1834,  ...,  0.3864,  0.2984, -0.2824],\n",
      "         [-0.2499,  0.5370,  0.3027,  ...,  0.5449, -0.0047, -0.3193],\n",
      "         [-0.1185,  0.1910,  0.2534,  ...,  0.2078,  0.2412, -0.3069]],\n",
      "\n",
      "        [[-0.5878,  0.1428,  0.3855,  ..., -0.0848,  0.0437, -0.2870],\n",
      "         [-0.2136, -0.0962,  0.4480,  ...,  0.2811,  0.1335, -0.0452],\n",
      "         [-0.0331,  0.2020,  0.3414,  ...,  0.1868, -0.1507, -0.1088],\n",
      "         ...,\n",
      "         [-0.5207,  0.4380,  0.2661,  ...,  0.3712,  0.2695, -0.5013],\n",
      "         [-0.4010,  0.5690,  0.3811,  ...,  0.4073, -0.0854, -0.1981],\n",
      "         [-0.2033,  0.7031,  0.2558,  ...,  0.1040,  0.3197, -0.4890]],\n",
      "\n",
      "        [[-0.1669,  0.5340,  0.2303,  ...,  0.0810, -0.2314, -0.3414],\n",
      "         [-0.0861, -0.0205,  0.4792,  ..., -0.0157,  0.0549, -0.0196],\n",
      "         [-0.1814,  0.4127,  0.5447,  ...,  0.1113, -0.0520, -0.4881],\n",
      "         ...,\n",
      "         [-0.3357,  0.6047,  0.2284,  ...,  0.4864,  0.2441, -0.5188],\n",
      "         [-0.4058,  0.5027,  0.0327,  ...,  0.3040,  0.0823, -0.2141],\n",
      "         [-0.3670,  0.2954,  0.1785,  ...,  0.3615,  0.1624, -0.1799]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n",
      "loss: 0.2423476129770279\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# let's start with a very simple model\n",
    "\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Returns the positional encoding for a given sequence length and model size.\n",
    "\n",
    "    Parameters:\n",
    "    - seq_len (int): Length of the sequence.\n",
    "    - d_model (int): Size of the model embedding.\n",
    "\n",
    "    Returns:\n",
    "    - A tensor of shape (seq_len, d_model) containing the positional encoding.\n",
    "    \"\"\"\n",
    "    \n",
    "    position = torch.arange(seq_len).unsqueeze(1).float() # [seq_len, 1]\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                         (-math.log(10000.0) / d_model))  # [d_model/2]\n",
    "    pos_enc = torch.zeros((seq_len, d_model))\n",
    "\n",
    "    pos_enc[:, 0::2] = torch.sin(position * div_term) # apply sin to even indices in the array; 2i\n",
    "    pos_enc[:, 1::2] = torch.cos(position * div_term) # apply cos to odd indices in the array; 2i+1\n",
    "\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self attention\"\"\"\n",
    "    \n",
    "    def __init__(self,head_size,n_emb,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False, device=device)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.n_emb = n_emb\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # batch size, time, context\n",
    "        # key, query, value\n",
    "        k = self.key(x) # B,T,C\n",
    "        q = self.query(x) # B,T,C\n",
    "        v= self.value(x) # B,T,C\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "         # Scaled dot-product attention - same as below\n",
    "        # attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / math.sqrt(self.embed_size)\n",
    "\n",
    "        wei = q @ k.transpose(-1,-2) # B,T,T\n",
    "        wei /= math.sqrt(self.n_emb) # scale by sqrt of embedding dimension\n",
    "        self.tril = self.tril.to(device)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # mask out upper triangular part so don't attend to future\n",
    "        wei = F.softmax(wei, dim=-1) # B,T,T\n",
    "        wei = self.dropout(wei)\n",
    "        # apply attention to values - weighted aggregation\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) --> B,T,C\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_heads,head_size,n_emb,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size,n_emb) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb, bias=False, device=device) # (B,T,C) - projection back into residual pathway\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is (B,T,C)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B,T,C*num_heads)\n",
    "        out = self.dropout(self.proj(out)) # (B,T,C) - projection back into residual pathway\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple lineear layer followed by a ReLU - allows all tokens to think on data individually\"\"\"\n",
    "    \n",
    "    def __init__(self,n_emb,dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb , device=device), # 4 * because recommended in paper residual pathway - growing residual pathway\n",
    "            nn.ReLU(),\n",
    "            nn.Linear( 4* n_emb, n_emb , device=device), # required otherwise output will collapse  - projection back into residual pathway\n",
    "            nn.Dropout(dropout)\n",
    "          \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block: communication followed by computation - basically self attention heads and feedforward\"\"\"\n",
    "\n",
    "    def __init__(self, n_emb, n_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "        head_size = n_emb//n_heads\n",
    "        self.sa = MultiHeadAttention(num_heads=n_heads, head_size=head_size, n_emb=n_emb)\n",
    "        self.ffwd = FeedForward(n_emb=n_emb)\n",
    "        self.ln1 =  nn.InstanceNorm1d(n_emb , device=device)\n",
    "        self.ln2 =  nn.InstanceNorm1d(n_emb, device=device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x + due to residual connection\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class MotionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256, n_layers=8):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=False, device=device)  #input to hidden dim\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim, bias=False,device=device)  #reshape hidden to output dim\n",
    "        self.positional_encoding = positional_encoding(seq_len=block_size, d_model=hidden_dim).to(device)\n",
    "        layers = [Block(n_emb=hidden_dim, n_heads=4) for _ in range(n_layers)]\n",
    "        layers.append(nn.InstanceNorm1d(hidden_dim, device=device))\n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "\n",
    "        self.lm_head = nn.Linear(hidden_dim, hidden_dim, bias=False, device=device)\n",
    "       \n",
    "    \n",
    "        \n",
    "    def forward(self, inputs, targets=None ,mask=None):\n",
    "        B,T,C = inputs.shape # batch size, time, context\n",
    "        \n",
    "        # fc1 transforms input into hidden dimension\n",
    "        x = self.fc1(inputs) # B,T,hidden dimension\n",
    "        # Add positional encoding\n",
    "       \n",
    "        x += positional_encoding(seq_len=T, d_model=self.hidden_dim).to(device) # positional_encoding = T,hidden dimension , added = B,T,hidden dimension\n",
    "        \n",
    "        x = self.blocks(x) # B,T,hidden dimension\n",
    "        x = self.lm_head(x) # B,T,hidden dimension\n",
    "        \n",
    "        # fc2 transforms hidden dimension into output dimension\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        \n",
    "        else:\n",
    "            B,T,C = inputs.shape # batch size, time, context\n",
    "            loss = F.mse_loss(logits, targets) # mse picked cause continous data\n",
    "            # adding mask to ignore 0,0 occlusions (-inf)\n",
    "            # if mask is None:\n",
    "            #     mask = (inputs != float('-inf')).all(dim=-1).float() \n",
    "              \n",
    "            # loss = F.mse_loss(logits * mask.unsqueeze(-1), targets * mask.unsqueeze(-1), reduction='sum') / mask.sum()\n",
    "\n",
    "        \n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self,inputs,max_new_tokens):\n",
    "        # inputs is (B,T) array of indices in current context\n",
    "        # get current prediction\n",
    "    \n",
    "        generated_sequence = inputs\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            cond_sequence = generated_sequence[:, -block_size:] # get the last block_size tokens from the generated sequence so positional doesn't run out\n",
    "            # don't actually need to do this cause positional is sinusoidal but just in case since model trained with blocksize\n",
    "            logits, _ = self(cond_sequence)\n",
    "            next_values = logits[:, -1, :]  # Get the values from the last timestep\n",
    "            \n",
    "            # Append the predicted values to the sequence\n",
    "            generated_sequence = torch.cat([generated_sequence, next_values.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        return generated_sequence\n",
    "\n",
    "# output dim should be the same size as target dim\n",
    "m = MotionModel(input_dim=57, output_dim=57)\n",
    "out,loss = m(xb, yb)\n",
    "print(out.shape)\n",
    "print(out)\n",
    "# loss interpreted on scale of data\n",
    "print(f\"loss: {loss}\")\n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "global train_seed\n",
    "train_seed = random.randint(1, 100000)\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, checkpoint_path):\n",
    "    \"\"\"Save the model checkpoint.\"\"\"\n",
    "    # Use the run seed in the filename\n",
    "    # checkpoint_path = os.path.join(checkpoint_dir, f\"MEED_checkpoint_{run_seed}.pth\")\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print('Creating checkpoints directory...')\n",
    "        os.makedirs(checkpoint_path)\n",
    "    \n",
    "    print(f\"Saving model checkpoint to {checkpoint_path}\")\n",
    "    state = {'model': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict(),\n",
    "             'epoch': epoch,\n",
    "             'loss': loss,\n",
    "             'train_seed' : train_seed}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\\notebooks\\prototypes\\inter-prototype-4.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m steps \u001b[39m%\u001b[39m \u001b[39m5000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     losses \u001b[39m=\u001b[39m estimate_loss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain loss: \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m val loss: \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.6f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# Store the losses for plotting\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\avika\\anaconda3\\envs\\interactive_dance_thesis\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\\notebooks\\prototypes\\inter-prototype-4.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     xb, yb, mask \u001b[39m=\u001b[39m get_batch(split,  block_size, batch_size, device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     logits, loss \u001b[39m=\u001b[39m m(xb, yb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m out[split] \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\avika\\anaconda3\\envs\\interactive_dance_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\\notebooks\\prototypes\\inter-prototype-4.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39m# Add positional encoding\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m positional_encoding(seq_len\u001b[39m=\u001b[39mT, d_model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_dim)\u001b[39m.\u001b[39mto(device) \u001b[39m# positional_encoding = T,hidden dimension , added = B,T,hidden dimension\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x) \u001b[39m# B,T,hidden dimension\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(x) \u001b[39m# B,T,hidden dimension\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39m# fc2 transforms hidden dimension into output dimension\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\avika\\anaconda3\\envs\\interactive_dance_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\avika\\anaconda3\\envs\\interactive_dance_thesis\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\avika\\anaconda3\\envs\\interactive_dance_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\\notebooks\\prototypes\\inter-prototype-4.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     \u001b[39m# x + due to residual connection\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msa(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(x))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\avika\\anaconda3\\envs\\interactive_dance_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\\notebooks\\prototypes\\inter-prototype-4.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39m# x is (B,T,C)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (B,T,C*num_heads)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out)) \u001b[39m# (B,T,C) - projection back into residual pathway\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[1;32mc:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\\notebooks\\prototypes\\inter-prototype-4.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39m# x is (B,T,C)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (B,T,C*num_heads)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out)) \u001b[39m# (B,T,C) - projection back into residual pathway\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\avika\\anaconda3\\envs\\interactive_dance_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\\notebooks\\prototypes\\inter-prototype-4.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# key, query, value\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(x) \u001b[39m# B,T,C\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(x) \u001b[39m# B,T,C\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m v\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(x) \u001b[39m# B,T,C\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# compute attention scores (\"affinities\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m  \u001b[39m# Scaled dot-product attention - same as below\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X30sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# attention = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / math.sqrt(self.embed_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\avika\\anaconda3\\envs\\interactive_dance_thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\avika\\anaconda3\\envs\\interactive_dance_thesis\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    eval_iters = 5000\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb, mask = get_batch(split,  block_size, batch_size, device)\n",
    "            logits, loss = m(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "TRAIN = True\n",
    "CHECKPOINT_PATH=\"checkpoints/proto4_checkpoint.pth\"\n",
    "if TRAIN == True:\n",
    "\n",
    "    epoch = 200000\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for steps in tqdm(range(epoch)):\n",
    "        # get sample batch of data\n",
    "        xb,yb,mask = get_batch('train',  block_size, batch_size, device)\n",
    "        # evaluate loss\n",
    "        logits, loss = m(xb,yb, mask)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % 5000 == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(f\"train loss: {losses['train']:.6f} val loss: {losses['val']:.6f}\")\n",
    "            \n",
    "            # Store the losses for plotting\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "    \n",
    "    save_checkpoint(model=m, optimizer=optimizer, epoch=epoch, loss=loss, checkpoint_path=CHECKPOINT_PATH)\n",
    "            \n",
    "    def plot_losses(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Steps (in thousands)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.show()\n",
    "\n",
    "    # After the training loop, plot the losses\n",
    "    plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path):\n",
    "    \"\"\"Load the model checkpoint.\"\"\"\n",
    "    print('Loading checkpoint...')\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['model'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    epoch = state['epoch']\n",
    "    loss = state['loss']\n",
    "    train_seed = state['train_seed']\n",
    "    print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "    return model, optimizer, epoch, loss,train_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Checkpoint loaded from checkpoints/proto4_checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# generate new sequence\n",
    "CHECKPOINT_PATH = \"checkpoints/proto4_checkpoint.pth\"\n",
    "\n",
    "m, optimizer, epoch, loss, train_seed = load_checkpoint(m, optimizer, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Checkpoint loaded from checkpoints/proto4_checkpoint.pth\n",
      "torch.Size([8, 310, 57])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "xb,yb,mask = get_batch('test', block_size, batch_size, device)\n",
    "\n",
    "generated = m.generate(xb, 300)\n",
    "print(generated.shape)\n",
    "# much faster from loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1:\n",
      "  Frame 1: Anger\n",
      "  Frame 2: Anger\n",
      "  Frame 3: Anger\n",
      "  Frame 4: Anger\n",
      "  Frame 5: Anger\n",
      "  Frame 6: Anger\n",
      "  Frame 7: Anger\n",
      "  Frame 8: Anger\n",
      "  Frame 9: Anger\n",
      "  Frame 10: Anger\n",
      "Sequence 2:\n",
      "  Frame 1: Anger\n",
      "  Frame 2: Anger\n",
      "  Frame 3: Anger\n",
      "  Frame 4: Anger\n",
      "  Frame 5: Anger\n",
      "  Frame 6: Anger\n",
      "  Frame 7: Anger\n",
      "  Frame 8: Anger\n",
      "  Frame 9: Anger\n",
      "  Frame 10: Anger\n",
      "Sequence 3:\n",
      "  Frame 1: Anger\n",
      "  Frame 2: Anger\n",
      "  Frame 3: Anger\n",
      "  Frame 4: Anger\n",
      "  Frame 5: Anger\n",
      "  Frame 6: Anger\n",
      "  Frame 7: Anger\n",
      "  Frame 8: Anger\n",
      "  Frame 9: Anger\n",
      "  Frame 10: Anger\n",
      "Sequence 4:\n",
      "  Frame 1: Anger\n",
      "  Frame 2: Anger\n",
      "  Frame 3: Anger\n",
      "  Frame 4: Anger\n",
      "  Frame 5: Anger\n",
      "  Frame 6: Anger\n",
      "  Frame 7: Anger\n",
      "  Frame 8: Anger\n",
      "  Frame 9: Anger\n",
      "  Frame 10: Anger\n",
      "Sequence 5:\n",
      "  Frame 1: Anger\n",
      "  Frame 2: Anger\n",
      "  Frame 3: Anger\n",
      "  Frame 4: Anger\n",
      "  Frame 5: Anger\n",
      "  Frame 6: Anger\n",
      "  Frame 7: Anger\n",
      "  Frame 8: Anger\n",
      "  Frame 9: Anger\n",
      "  Frame 10: Anger\n",
      "Sequence 6:\n",
      "  Frame 1: Anger\n",
      "  Frame 2: Anger\n",
      "  Frame 3: Anger\n",
      "  Frame 4: Anger\n",
      "  Frame 5: Anger\n",
      "  Frame 6: Anger\n",
      "  Frame 7: Anger\n",
      "  Frame 8: Anger\n",
      "  Frame 9: Anger\n",
      "  Frame 10: Anger\n",
      "Sequence 7:\n",
      "  Frame 1: Anger\n",
      "  Frame 2: Anger\n",
      "  Frame 3: Anger\n",
      "  Frame 4: Anger\n",
      "  Frame 5: Anger\n",
      "  Frame 6: Anger\n",
      "  Frame 7: Anger\n",
      "  Frame 8: Anger\n",
      "  Frame 9: Anger\n",
      "  Frame 10: Anger\n",
      "Sequence 8:\n",
      "  Frame 1: Anger\n",
      "  Frame 2: Anger\n",
      "  Frame 3: Anger\n",
      "  Frame 4: Anger\n",
      "  Frame 5: Anger\n",
      "  Frame 6: Anger\n",
      "  Frame 7: Anger\n",
      "  Frame 8: Anger\n",
      "  Frame 9: Anger\n",
      "  Frame 10: Anger\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xb,yb,mask = get_batch('train', block_size, batch_size, device)\n",
    "# Define a mapping from emotion vectors to emotion labels\n",
    "# Define a mapping from emotion vectors to emotion labels\n",
    "import torch\n",
    "\n",
    "# Define a mapping from emotion vectors to emotion labels\n",
    "vector_to_label = {\n",
    "    (1, 0, 0, 0, 0, 0, 0): 'Anger',\n",
    "    (0, 1, 0, 0, 0, 0, 0): 'Disgust',\n",
    "    (0, 0, 1, 0, 0, 0, 0): 'Fear',\n",
    "    (0, 0, 0, 1, 0, 0, 0): 'Happiness',\n",
    "    (0, 0, 0, 0, 1, 0, 0): 'Neutral',\n",
    "    (0, 0, 0, 0, 0, 1, 0): 'Sad',\n",
    "    (0, 0, 0, 0, 0, 0, 1): 'Surprise'\n",
    "}\n",
    "\n",
    "def display_emotions(xb):\n",
    "    # Ensure xb is on the CPU and convert to numpy for easier indexing\n",
    "    xb_np = xb.cpu().numpy()\n",
    "    \n",
    "    # Iterate through each sequence and frame in xb\n",
    "    for i, sequence in enumerate(xb_np):\n",
    "        print(f\"Sequence {i+1}:\")\n",
    "        for j, frame in enumerate(sequence):\n",
    "            # Extract the emotion vector (assumed to be the last 7 elements)\n",
    "            emotion_vector = tuple(frame[-7:].astype(int))\n",
    "            \n",
    "            # Map the emotion vector to its corresponding label\n",
    "            emotion_label = vector_to_label.get(emotion_vector, 'Unknown')\n",
    "            \n",
    "            # Display the emotion label\n",
    "            print(f\"  Frame {j+1}: {emotion_label}\")\n",
    "\n",
    "# Usage example:\n",
    "# xb = ...  # Your data\n",
    "\n",
    "\n",
    "\n",
    "display_emotions(xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\\notebooks\\prototypes\\inter-prototype-4.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X33sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# visualise and save\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X33sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m unnorm_out:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X33sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     visualise_skeleton(batch, max_x, max_y, max_frames\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m,save \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,save_path\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,prefix\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39madam_10000steps_proto4\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\avika\\OneDrive\\Documents\\UAL\\interactive_dance_thesis\\notebooks\\prototypes\\inter-prototype-4.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X33sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     out\u001b[39m.\u001b[39mwrite(canvas_copy)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X33sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m# Wait for 100ms and check for \"esc\" key press to exit\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X33sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m key \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m100\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X33sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39m27\u001b[39m:  \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/avika/OneDrive/Documents/UAL/interactive_dance_thesis/notebooks/prototypes/inter-prototype-4.ipynb#X33sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def unnormalise_list(data_tensor, max_x, min_x, max_y, min_y):\n",
    "    all_frames = []\n",
    "    # Loop through each batch\n",
    "    for batch_idx in range(data_tensor.size(0)):\n",
    "        batch_frames = []\n",
    "        # Loop through each frame in the batch\n",
    "        for frame_idx in range(data_tensor.size(1)):\n",
    "            frame_data = data_tensor[batch_idx, frame_idx, :]\n",
    "            unnormalized_data = []\n",
    "            # Loop through the coordinate pairs and unnormalize\n",
    "            for i in range(0, 50, 2):  \n",
    "                x = frame_data[i]\n",
    "                y = frame_data[i+1]\n",
    "                unnormalized_x = (x+1)/2 * (max_x-min_x) + min_x\n",
    "                unnormalized_y = (y+1)/2 * (max_y-min_y) + min_y\n",
    "                unnormalized_data.extend([unnormalized_x.item(), unnormalized_y.item()])\n",
    "            # Append the emotion encoding without unnormalizing\n",
    "            unnormalized_data.extend(frame_data[-7:].tolist())\n",
    "            batch_frames.append(unnormalized_data)\n",
    "        all_frames.append(batch_frames)\n",
    "    return all_frames\n",
    "\n",
    "\n",
    "\n",
    "unnorm_out = unnormalise_list(generated, max_x, min_x, max_y, min_y)\n",
    "\n",
    "# visualise and save\n",
    "for batch in unnorm_out:\n",
    "    visualise_skeleton(batch, max_x, max_y, max_frames=300,save = True,save_path=None,prefix=f'adam_{epoch}steps_proto4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # continuous GPT\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import functional as F\n",
    "# import math\n",
    "\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "# # this is set to eval mode because we don't want to train the model, we just want to estimate the loss, for this model the modes won't be different\n",
    "# # but for other models, the modes will be different depending on what layers are present\n",
    "# # torch.no_grad() - we don't want to calculate gradients because we don't want to train the model, we will not call backward, better memory management\n",
    "# @torch.no_grad()\n",
    "# def estimate_loss():\n",
    "#     out = {}\n",
    "#     model.eval()\n",
    "#     eval_iters = 200\n",
    "#     for split in ['train', 'val']:\n",
    "#         losses = torch.zeros(eval_iters)\n",
    "#         for k in range(eval_iters):\n",
    "#             xb, yb = get_batch(split)\n",
    "#             logits, loss = model(xb, yb)\n",
    "#             losses[k] = loss.item()\n",
    "#         out[split] = losses.mean()\n",
    "#     model.train()\n",
    "#     return out\n",
    "\n",
    "# def positional_encoding(seq_len, d_model):\n",
    "#     \"\"\"\n",
    "#     Returns the positional encoding for a given sequence length and model size.\n",
    "\n",
    "#     Parameters:\n",
    "#     - seq_len (int): Length of the sequence.\n",
    "#     - d_model (int): Size of the model embedding.\n",
    "\n",
    "#     Returns:\n",
    "#     - A tensor of shape (seq_len, d_model) containing the positional encoding.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     position = torch.arange(seq_len).unsqueeze(1).float() # [seq_len, 1]\n",
    "#     div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "#                          (-math.log(10000.0) / d_model))  # [d_model/2]\n",
    "#     pos_enc = torch.zeros((seq_len, d_model))\n",
    "\n",
    "#     pos_enc[:, 0::2] = torch.sin(position * div_term) # apply sin to even indices in the array; 2i\n",
    "#     pos_enc[:, 1::2] = torch.cos(position * div_term) # apply cos to odd indices in the array; 2i+1\n",
    "\n",
    "#     return pos_enc\n",
    "\n",
    "\n",
    "\n",
    "# class MotionModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim, hidden_dim, num_heads, num_layers):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # Initial transformation layer\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "#         # Transformer layers\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads)\n",
    "#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "#         # Positional encoding\n",
    "#         self.positional_encoding = positional_encoding(seq_len=block_size, d_model=hidden_dim).to(device)\n",
    "\n",
    "#         # Output layer\n",
    "#         self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "#     def forward(self, pose_sequence,targets=None):\n",
    "#         # Feature transformation\n",
    "#         x = self.fc1(pose_sequence)\n",
    "\n",
    "#         # Add positional encoding\n",
    "#         seq_len = x.shape[1]\n",
    "#         x += self.positional_encoding[:seq_len, :]\n",
    "\n",
    "\n",
    "#         # Transformer layers\n",
    "#         x = self.transformer(x)\n",
    "\n",
    "#         # Predicting the next pose\n",
    "#         logits = self.fc2(x)\n",
    "        \n",
    "        \n",
    "#         if targets is None:\n",
    "#             loss = None\n",
    "#         else:\n",
    "#             B, T, C = logits.shape\n",
    "#             # pytorch wants (B*T,C) so we have to transpose because it wants C in the 2nd dimension\n",
    "#             logits = logits.view(B*T, C)\n",
    "#             # look at prediction\n",
    "#             targets = targets.view(B*T, -1)\n",
    "\n",
    "#             # evaluate loss\n",
    "#             # negative log likelihood loss a.k. cross entropy loss\n",
    "#             # we have the identity of the next character so how well are we predicting the next character based on the logits\n",
    "#             # ideally the correct logits should be 1 and the rest should be 0, but in reality this is not the case\n",
    "            \n",
    "#             loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "#         return logits,loss\n",
    "\n",
    "#     def training_step(self, pose_sequence, next_pose):\n",
    "#         predicted_pose = self(pose_sequence)\n",
    "#         loss = nn.MSELoss()(predicted_pose, next_pose)\n",
    "#         return loss\n",
    "    \n",
    "#     def generate(self, initial_pose_sequence, max_new_poses):\n",
    "#         \"\"\"\n",
    "#         Generate a sequence of poses.\n",
    "\n",
    "#         Parameters:\n",
    "#         - initial_pose_sequence: Starting sequence of poses.\n",
    "#         - max_new_poses: Maximum number of new poses to generate.\n",
    "\n",
    "#         Returns:\n",
    "#         - Generated sequence of poses.\n",
    "#         \"\"\"\n",
    "\n",
    "#         generated_sequence = initial_pose_sequence\n",
    "\n",
    "#         for _ in range(max_new_poses):\n",
    "#             # Get the predicted next pose\n",
    "#             logits, _ = self(generated_sequence)\n",
    "#             next_pose = logits\n",
    "\n",
    "\n",
    "#             # Append the predicted pose to the sequence\n",
    "#             generated_sequence = torch.cat([initial_pose_sequence,next_pose],dim=1)  # Add sequence dimension\n",
    "\n",
    "\n",
    "#         return generated_sequence\n",
    "\n",
    "\n",
    "\n",
    "# model = MotionModel(input_dim=50, hidden_dim=128, num_heads=4, num_layers=4)\n",
    "# m = model.to(device)\n",
    "# logits, loss = m(xb, yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "# output= m.generate(initial_pose_sequence=xb[0], max_new_poses=100)[0].tolist()\n",
    "# print(np.shape(output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the MEED dataset for transformer\n",
    "# # 1D array of 50 (25 keypoints x,y ) = 1 frame = 1 token\n",
    "# # block size of 10 so input is 10x50 dims\n",
    "\n",
    "# from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# # Example: load your keypoints from a file, preprocess, and convert them into PyTorch tensors\n",
    "# # keypoints = ...\n",
    "# # For the sake of this example, let's assume keypoints is a torch.Tensor of shape [num_samples, sequence_len, 50]\n",
    "\n",
    "# # Create data loaders\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(xb, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(yb, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interactive_dance_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
